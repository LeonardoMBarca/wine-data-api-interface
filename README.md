# ğŸ“¦ Tech Challenge FIAP Machine Learning API

Este projeto Ã© uma **API REST** desenvolvida com **Flask**, utilizando **autenticaÃ§Ã£o JWT** para proteger seus endpoints. A estrutura foi organizada seguindo boas prÃ¡ticas de modularidade, facilitando a manutenÃ§Ã£o e escalabilidade do sistema. A API realiza **coleta automatizada de dados da Embrapa** e expÃµe os dados jÃ¡ tratados para consumo por aplicaÃ§Ãµes externas ou modelos de machine learning.

## âš™ï¸ Funcionalidades

### ğŸ” AutenticaÃ§Ã£o

* **Endpoint:** `POST /auth/login`
* Gera tokens JWT com validade de **30 minutos** para autenticaÃ§Ã£o de usuÃ¡rios.

### ğŸ•·ï¸ Coleta e Processamento de Dados (Crawler)

* **Endpoint:** `POST /crawler/executar`
* Realiza:

  1. Download dos dados brutos do site da Embrapa (camada Bronze)
  2. Limpeza e padronizaÃ§Ã£o (camada Silver)
  3. Curadoria final e dados prontos para uso (camada Gold)

### ğŸ“Š Consulta PÃºblica de Dados

* **Endpoint:** `GET /<categoria>/<subcategoria>`
* Exemplo: `/exportacao/suco`, `/producao/default`
* Consulta os dados atualizados diretamente da camada Gold
* O crawler Ã© executado automaticamente antes de servir os dados ğŸ’¡

---

## **1. ConfiguraÃ§Ã£o Inicial**

### **1.1. Clonar o RepositÃ³rio**

```bash
git clone <url-do-repositorio>
cd tech-challenge-fiap-machine-learning-api
```

### **1.2. Criar e Ativar o Ambiente Virtual**

#### Windows

```bash
python -m venv venv
.venv\Scripts\activate
```

#### Linux/Mac

```bash
python -m venv venv
source venv/bin/activate
```

### **1.3. Instalar DependÃªncias**

```bash
pip install -r requirements.txt
```

### **1.4. ConfiguraÃ§Ã£o de VariÃ¡veis de Ambiente**

Crie um arquivo `.env` com:

```bash
JWT_SECRET_KEY=<sua_chave_secreta>
```

---

## **2. Executar a API**

### **2.1. Iniciar o Servidor**

```bash
python -m api.app
```

API disponÃ­vel em: `http://127.0.0.1:5000`

Acesse a documentaÃ§Ã£o Swagger UI: `http://127.0.0.1:5000/apidocs`

---

## **3. AutenticaÃ§Ã£o**

### **3.1. Endpoint de Login**

```bash
POST /auth/login
```

#### Body (JSON):

```json
{
  "username": "user",
  "password": "user"
}
```

#### Exemplo de resposta:

```json
{
  "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
}
```

---

## **4. Executar Crawler Manualmente**

### **4.1. Endpoint Protegido**

```bash
POST /crawler/executar
```

Requer header:

```http
Authorization: Bearer <seu_token>
```

---

## **5. Consultar Dados Tratados (Gold Layer)**

### **5.1. Exemplo de endpoint pÃºblico:**

```bash
GET /exportacao/suco
GET /importacao/espumantes
GET /producao/default
```

#### Requer header:

```http
Authorization: Bearer <seu_token>
```

A cada consulta, o pipeline completo Ã© executado automaticamente (para fins de demonstraÃ§Ã£o do MVP).

#### Exemplo de resposta:

```json
{
  "mensagem": "Dados de exportacao/suco carregados com sucesso!",
  "usuario": "user",
  "dados": [
    {"pais": "alemanha", "ano": 1970, "kilograms": 0, "dollars": 0},
    {"pais": "brasil", "ano": 1971, "kilograms": 100, "dollars": 2000}
  ]
}
```

---

## **6. Estrutura do Projeto**

```
tech-challenge-fiap-machine-learning-api/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ auth.py
â”‚   â”‚   â”œâ”€â”€ crawler.py
â”‚   â”‚   â””â”€â”€ data.py
â”œâ”€â”€ crawler/
â”‚   â”œâ”€â”€ scraper/
â”‚   â”‚   â””â”€â”€ downloader.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ etl/
â”‚   â”‚   â”œâ”€â”€ bronze_to_silver/
â”‚   â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”‚   â””â”€â”€ silver_to_gold/
â”‚   â”‚       â””â”€â”€ main.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bronze-layer/
â”‚   â”œâ”€â”€ silver-layer/
â”‚   â””â”€â”€ gold-layer/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .env
```

---

## ğŸš€ CenÃ¡rio de AplicaÃ§Ã£o com Machine Learning

Esta API pode alimentar modelos de previsÃ£o de:

* ExportaÃ§Ãµes futuras de vinhos ou sucos
* ProduÃ§Ã£o estimada para o prÃ³ximo ano
* AnÃ¡lise de sazonalidade e demanda

Em cenÃ¡rios reais, a atualizaÃ§Ã£o automÃ¡tica do pipeline seria feita por agendamento (Jenkins, cron, Airflow), e nÃ£o a cada consulta.

Para este MVP, **cada chamada executa o pipeline para garantir dados atualizados e demonstrar o fluxo completo.**

---


